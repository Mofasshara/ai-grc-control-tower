# AI Incident & Hallucination Management

An AI incident is any event where an AI system:
- Produces incorrect or misleading output
- Generates hallucinated information
- Produces unsafe, biased, or non-compliant content
- Behaves unpredictably or outside its intended purpose
- Fails to follow defined guardrails or governance rules

AI incidents must be logged, assessed, investigated, and resolved using a structured workflow.

---

## Why This Policy Exists
AI systems can influence decisions in regulated environments.  
This policy ensures:
- Early detection of harmful or incorrect AI behavior  
- Consistent handling of issues  
- Clear documentation for auditors  
- Prevention of repeated failures  
- Protection of customers, patients, and the organization  

---

## Incident Types

### **1. Hallucination**
The AI invents facts, data, or events that are not real.

### **2. Incorrect Factual Output**
The AI provides wrong information even though the correct information exists.

### **3. Policy Violation**
The AI produces content that violates:
- Regulatory rules  
- Internal policies  
- Ethical guidelines  

### **4. Bias / Fairness Issue**
The AI produces output that is discriminatory or unfair toward a group or individual.

### **5. Unsafe Recommendation**
The AI suggests actions that could cause:
- Harm  
- Financial loss  
- Compliance breaches  
- Patient or customer risk  

---

## Simple Explanation (Non-Technical)
This document answers the auditor's question:

**"What do you consider an AI incident?"**

It defines the boundaries of what is acceptable and what must be investigated.

Non-technical explanation of Step 1:
Think of this like writing the "What counts as a fire?" section of a fire safety manual.

Before you can respond to incidents, you must define:

What is a fire

What is smoke

What is a false alarm

Similarly, before you can manage AI incidents, you must define:

What is a hallucination

What is unsafe

What is biased

What is non-compliant

This gives auditors confidence that you have a clear, shared definition across your organization.

## Severity Levels

### **Low Severity**
- No business impact  
- Issue caught internally before reaching users  
- Minimal risk  
- No regulatory exposure  

### **Medium Severity**
- Incorrect or misleading output reached users  
- Minor customer impact  
- Requires investigation and corrective action  
- No immediate regulatory threat  

### **High Severity**
- Potential regulatory, patient, or customer impact  
- Could cause financial loss, safety risk, or compliance breach  
- Requires urgent escalation and formal investigation  
- Must be reported to Compliance and Risk teams  

## Impact Areas

AI incidents must be evaluated across the following dimensions:

- **Regulatory compliance**  
  Could this violate FINMA, GxP, GDPR, or internal policies?

- **Customer impact**  
  Did the AI harm or mislead a customer?

- **Patient safety**  
  Did the AI produce unsafe medical or health-related guidance?

- **Financial risk**  
  Could this cause monetary loss or incorrect financial decisions?

- **Reputational risk**  
  Could this damage trust in the organization or product?
